# Hugging Face Katzendetector-Playground

Dieses Projekt verwendet das Hugging Face OWLv2 (Open-World Localization) Modell zur automatischen Erkennung von Katzen und Hunden in Videoaufnahmen. Das System analysiert Videomaterial durch Bildsequenz-Extraktion und KI-basierte Objekterkennung.

## Allgemeines Vorgehen

### 1. Video-zu-Bildsequenz Konvertierung
Das Projekt nutzt einen zweistufigen Ansatz zur effizienten Verarbeitung von Videodateien:

#### Einzelne Video-Konvertierung (`convert_video_to_image_sequence.py`)
- Konvertiert einzelne MP4-Videos in Bildsequenzen (JPG-Dateien)
- Extrahiert Frames mit konfigurierbaren Parametern (Qualit√§t, Frame-Skip)
- Speichert Bilder in einem `_frames` Unterordner neben der Videodatei
- Fortschrittsanzeige und Performance-Optimierung

#### Batch-Verarbeitung (`convert_video_batch.py`)
- Scannt rekursiv ein Verzeichnis nach allen Videodateien
- Pr√ºft automatisch, ob bereits `_frames` Ordner existieren
- Konvertiert automatisch alle Videos ohne entsprechende Bildsequenzen
- √úbersicht und Zusammenfassung aller verarbeiteten Dateien

### 2. KI-basierte Objekterkennung (`ai-processor.py`)

#### Verf√ºgbare AI-Modelle
Das Script unterst√ºtzt zwei verschiedene Objekterkennungsmodelle:

**OWLv2 (Standard)**
- **Modell**: `google/owlv2-base-patch16-ensemble`
- **Framework**: Hugging Face Transformers mit PyTorch
- **Erkennungskategorien**: Nur Katzen (text-basierte Erkennung)
- **Vorteile**: Flexiblere Klassendefinition, bessere Text-basierte Objekterkennung
- **Nachteile**: Langsamere Verarbeitung

**YOLO v11/v12 (Optional)**
- **Modelle**: `yolo11n.pt`, `yolo11s.pt`, `yolo11m.pt`, `yolo11l.pt`, `yolo11x.pt`
- **Framework**: Ultralytics YOLO
- **Erkennungskategorien**: Nur Katzen (aus 80 COCO-Klassen)
- **Vorteile**: Deutlich schnellere Verarbeitung, bessere Performance
- **Nachteile**: Feste Klassendefinition

#### Modell-Auswahl
```bash
# Standard OWLv2 verwenden
python ai-processor.py /pfad/zum/video.mp4

# YOLO verwenden (schneller)
python ai-processor.py --model yolo /pfad/zum/video.mp4

# Spezifisches YOLO-Modell verwenden
python ai-processor.py --model yolo --yolo-model yolo11l.pt /pfad/zum/video.mp4
```

#### YOLO-Modell √úbersicht
| Modell | Gr√∂√üe | Geschwindigkeit | Genauigkeit | Verwendung |
|--------|-------|----------------|-------------|------------|
| `yolo11n.pt` | Nano | Sehr schnell | Basis | Schnelle Tests |
| `yolo11s.pt` | Small | Schnell | Gut | Produktive Nutzung |
| `yolo11m.pt` | Medium | Mittel | Sehr gut | Ausgewogene L√∂sung |
| `yolo11l.pt` | Large | Langsam | Ausgezeichnet | Hohe Genauigkeit |
| `yolo11x.pt` | Extra Large | Sehr langsam | Beste | Maximale Pr√§zision |

#### Erkennungsprozess
1. **Frame-Laden**: Sequenzielle Verarbeitung der extrahierten Bildsequenz
2. **KI-Analyse**: Object Detection mit konfigurierbarem Schwellenwert (0.1)
3. **Bounding Box Extraktion**: Pr√§zise Lokalisierung erkannter Objekte
4. **Confidence-Bewertung**: Vertrauensscore f√ºr jede Erkennung

#### Ausgabe und Dokumentation
- **JSON-Export**: Strukturierte Speicherung aller Erkennungsergebnisse
- **Metadaten**: Verarbeitungsstatistiken und Konfigurationsparameter (inklusive verwendetes Modell)
- **Detaillierte Logs**: Fortschrittsanzeige mit Zeitsch√§tzungen
- **Performance-Metriken**: Detektionsrate und Verarbeitungsgeschwindigkeit
- **Modell-spezifische Ordner**: Getrennte Speicherung je nach verwendetem AI-Modell

#### Ordnerstruktur der Ergebnisse
```
/pfad/zum/video/
‚îú‚îÄ‚îÄ video.mp4
‚îú‚îÄ‚îÄ detection_results-owlv2/     # OWLv2 Ergebnisse
‚îÇ   ‚îî‚îÄ‚îÄ video_detection_results.json
‚îî‚îÄ‚îÄ detection_results-yolo/      # YOLO Ergebnisse
    ‚îî‚îÄ‚îÄ video_detection_results.json
```

Dies erm√∂glicht parallele Verarbeitung mit beiden Modellen und direkten Vergleich der Ergebnisse.

### 3. Datenstruktur der Ergebnisse

Die generierten JSON-Dateien enthalten:
- **Metadaten**: Timestamp, Verarbeitungsstatistiken, Konfiguration
- **Frame-Informationen**: Bildgr√∂√üe, Dateiname, Pfad
- **Erkennungsdetails**: Objekt-Label, Confidence-Score, Bounding Box Koordinaten

### 4. Systemanforderungen

#### Basis-Requirements (f√ºr OWLv2)
```
transformers>=4.30.0
torch>=2.0.0
torchvision>=0.15.0
Pillow>=9.0.0
scipy
opencv-python>=4.8.0
```

#### Zus√§tzliche Requirements f√ºr YOLO-Unterst√ºtzung
```
ultralytics>=8.0.0
numpy>=1.21.0
```

**Installation mit YOLO-Unterst√ºtzung:**
```bash
pip install -r requirements.txt
```

## Installation

Um die Scripts auf einem neuen Rechner zu installieren, folgen Sie diesen Schritten:

### Voraussetzungen
- Python 3.8+ installiert
- Git installiert
- Zugang zum Repository

### Schritt-f√ºr-Schritt Installation

1. **Repository klonen**
   ```bash
   git clone <repository-url>
   cd huggingface-catdetector-playground
   ```

2. **Virtuelle Umgebung erstellen**
   ```bash
   python -m venv venv
   ```

3. **Virtuelle Umgebung aktivieren**
   
   **Auf Linux/macOS:**
   ```bash
   source venv/bin/activate
   ```
   
   **Auf Windows:**
   ```bash
   venv\Scripts\activate
   ```

4. **Dependencies installieren**
   ```bash
   pip install -r requirements.txt
   ```

5. **Installation √ºberpr√ºfen**
   ```bash
   python -c "import torch; import transformers; print('Installation erfolgreich!')"
   ```

### Zus√§tzliche Hinweise f√ºr GPU-Unterst√ºtzung

F√ºr optimale Performance auf dem starken Rechner (Schritt 5 des Workflows):

```bash
# F√ºr CUDA-f√§hige GPUs (NVIDIA)
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121

# Installation √ºberpr√ºfen
python -c "import torch; print(f'CUDA verf√ºgbar: {torch.cuda.is_available()}')"
```

### Pfad-Anpassungen

Nach der Installation m√ºssen die Pfade in den Scripts an Ihre lokale Umgebung angepasst werden:

- **`convert_video_batch.py`**: Zeile 103 - Pfad zum Camera_Teich-Footage Ordner
- **`ai-processor.py`**: Zeile 14 - Pfad zum frames_folder

### Updates abrufen

Um das Projekt auf den neuesten Stand zu bringen:

```bash
git pull origin main
pip install -r requirements.txt  # Falls neue Dependencies hinzugef√ºgt wurden
```

## Funktionales Vorgehen

Der komplette Workflow des Katzendetector-Systems folgt einem strukturierten 6-Stufen-Prozess:

### 1. Video-Download
- **Aktion**: Benutzer l√§dt Videomaterial (z.B. von Kameras) herunter
- **Format**: Typischerweise MP4-Dateien
- **Speicherort**: Lokaler Rechner f√ºr erste Sichtung

### 2. Manuelle Video-Sichtung
- **Aktion**: Benutzer sichtet das heruntergeladene Video manuell
- **Entscheidung**: 
  - ‚úÖ **Katzen vorhanden** ‚Üí Video behalten f√ºr weitere Verarbeitung
  - ‚ùå **Keine Katzen** ‚Üí Video l√∂schen (spart Speicher und Verarbeitungszeit)
- **Zweck**: Effiziente Vorselektion zur Reduzierung der Datenmenge

### 3. Video-Transfer auf Fileshare
- **Aktion**: Relevante Videos (mit Katzen) auf zentralen Fileshare √ºbertragen
- **Ziel**: Zentrale Verf√ºgbarkeit f√ºr die weitere automatisierte Verarbeitung
- **Organisation**: Strukturierte Ablage nach Datum/Kamera/etc.

### 4. Automatische Bildsequenz-Extraktion
- **Tool**: `convert_video_batch.py`
- **Ausf√ºhrungsort**: Fileshare-Server
- **Prozess**: 
  - Rekursives Scannen aller Videos im Fileshare
  - Automatische Konvertierung in Bildsequenzen (`_frames` Ordner)
  - Vermeidung doppelter Verarbeitung bereits konvertierter Videos

### 5. KI-basierte Objekterkennung
- **Tool**: `ai-processor.py`
- **Ausf√ºhrungsort**: Starker Rechner mit GPU-Unterst√ºtzung
- **Prozess**:
  - Verarbeitung der generierten Bildsequenzen
  - Wahlweise OWLv2 oder YOLO f√ºr Objekterkennung
  - Erzeugung detaillierter JSON-Erkennungsberichte in modell-spezifischen Ordnern

#### Performance-Empfehlungen:
- **Gro√üe Datenmengen**: YOLO verwenden (`--model yolo`) f√ºr deutlich schnellere Verarbeitung
- **H√∂chste Genauigkeit**: OWLv2 f√ºr pr√§zisere text-basierte Katzenerkennung
- **GPU-Nutzung**: Beide Modelle profitieren von CUDA/MPS-Unterst√ºtzung
- **Modell-Vergleich**: Parallele Verarbeitung mit beiden Modellen f√ºr Qualit√§tsvergleich

### 6. Zentrale Zusammenf√ºhrung der Ergebnisse
- **Aktion**: Alle generierten JSON-Files werden an zentraler Stelle gesammelt
- **Zweck**: 
  - √úbergreifende Analyse und Statistiken
  - Zentrale Datenhaltung f√ºr weitere Auswertungen
  - Langzeit-Monitoring und Trends

## Technischer Workflow

F√ºr die technische Umsetzung:

1. **Video-Vorbereitung**: Platzierung der selektierten MP4-Dateien im Fileshare-Verzeichnis
2. **Batch-Konvertierung**: Ausf√ºhrung von `convert_video_batch.py` f√ºr automatische Bildsequenz-Extraktion
3. **KI-Analyse**: Ausf√ºhrung der Objekterkennung auf leistungsstarker Hardware
   ```bash
   # Schnelle Verarbeitung mit YOLO
   python ai-processor.py --model yolo /pfad/zu/video_frames/
   
   # Pr√§zise Verarbeitung mit OWLv2
   python ai-processor.py --model owlv2 /pfad/zu/video_frames/
   
   # Beide Modelle f√ºr Vergleich
   python ai-processor.py --model owlv2 /pfad/zu/video_frames/
   python ai-processor.py --model yolo /pfad/zu/video_frames/
   ```
4. **Ergebnis-Aggregation**: Sammlung und Zusammenf√ºhrung aller JSON-Ausgabedateien aus modell-spezifischen Ordnern

Das System ist f√ºr gro√üe Videodatenmengen optimiert und bietet eine vollst√§ndig automatisierte Pipeline von der manuell vorselektierten Video-Eingabe bis zur strukturierten Erkennungsausgabe. Die YOLO-Integration erm√∂glicht deutlich schnellere Verarbeitung gro√üer Datenmengen bei weiterhin hoher Erkennungsqualit√§t.

## Detektions-Annotation Script

### √úbersicht

Das `annotate_detections.py` Script liest JSON-Detektionsergebnisse von `ai-processor.py` ein und zeichnet Bounding-Boxes auf die entsprechenden Frame-Bilder.

### Funktionen

#### Hauptfunktionen:
- ‚úÖ **JSON-Parser**: L√§dt Detektionsergebnisse aus JSON-Dateien
- ‚úÖ **Bounding-Box Zeichnung**: Zeichnet farbige Bounding-Boxes um erkannte Objekte
- ‚úÖ **Label-Anzeige**: Zeigt Label und Confidence-Score f√ºr jede Detektion
- ‚úÖ **Batch-Verarbeitung**: Verarbeitet alle detektierten Frames automatisch
- ‚úÖ **Fortschrittsanzeige**: Zeigt Verarbeitungsfortschritt in Echtzeit
- ‚úÖ **√úbersichtsbild**: Erstellt automatisch ein Grid der besten Detektionen

#### Farbkodierung:
- üü† **Orange**: Katzen (`cat`)
- üü¢ **Gr√ºn**: Hunde (`dog`)
- üî¥ **Rot**: Personen (`person`)
- üîµ **Blau**: V√∂gel (`bird`)
- üü° **Gelb**: Andere/Unbekannte Labels

### Verwendung

#### Grundlegende Verwendung:
```bash
python annotate_detections.py detection_results/Camera_Teich-20250907-155316-1757253196613-7_frames_detection_results.json
```

#### Mit benutzerdefiniertem Ausgabeordner:
```bash
python annotate_detections.py detection_results.json --output-dir meine_annotationen
```

#### Mit OpenCV-Rendering (Alternative zu PIL):
```bash
python annotate_detections.py detection_results.json --opencv
```

#### Ohne √úbersichtsbild:
```bash
python annotate_detections.py detection_results.json --no-summary
```

### Parameter

| Parameter | Beschreibung | Standard |
|-----------|--------------|----------|
| `json_file` | Pfad zur JSON-Datei mit Detektionsergebnissen | (erforderlich) |
| `--output-dir`, `-o` | Ausgabeordner f√ºr annotierte Bilder | `annotated_<json_basename>` |
| `--opencv` | Verwendet OpenCV statt PIL f√ºr Rendering | PIL |
| `--no-summary` | Erstellt kein √úbersichtsbild | √úbersichtsbild erstellen |

### Ausgabe

Das Script erstellt:

1. **Annotierte Einzelbilder**: Jeder Frame mit Detektionen wird mit Bounding-Boxes versehen
   - Format: `annotated_frame_XXXXXX.jpg`
   - Enth√§lt alle Detektionen mit Label und Confidence

2. **√úbersichtsbild**: `detection_summary.jpg`
   - Grid-Layout der besten Detektionen (h√∂chste Confidence)
   - Bis zu 9 Bilder in 3x3 Anordnung
   - Thumbnail-Gr√∂√üe f√ºr schnelle √úbersicht

### Beispiel-Workflow

1. **Video analysieren** (mit ai-processor.py):
   ```bash
   python ai-processor.py mein_video.mp4
   # Erstellt: detection_results/mein_video_detection_results.json
   ```

2. **Detektionen annotieren**:
   ```bash
   python annotate_detections.py detection_results/mein_video_detection_results.json
   # Erstellt: annotated_mein_video_detection_results/
   ```

3. **Ergebnisse betrachten**:
   - Einzelne Frames: `annotated_mein_video_detection_results/annotated_frame_*.jpg`
   - √úbersicht: `annotated_mein_video_detection_results/detection_summary.jpg`

### Technische Details

#### Unterst√ºtzte Formate:
- **Input**: JSON-Dateien von ai-processor.py
- **Output**: JPEG-Bilder mit 95% Qualit√§t
- **Frame-Formate**: Alle von PIL/OpenCV unterst√ºtzten Formate

#### Performance:
- **PIL-Modus**: Bessere Qualit√§t, etwas langsamer
- **OpenCV-Modus**: Schneller, f√ºr gro√üe Mengen geeignet
- **Speicher**: Effizient, l√§dt nur jeweils ein Bild

#### Fehlerbehandlung:
- Fehlende Frame-Bilder werden √ºbersprungen
- Korrupte JSON-Dateien werden erkannt
- Detaillierte Fehlermeldungen f√ºr Debugging

### Anpassungen

Das Script kann leicht angepasst werden:

- **Farben √§ndern**: Bearbeiten Sie die `get_label_color()` Funktion
- **Text-Stil**: Anpassen von `font_size`, `box_thickness` etc.
- **Neue Labels**: Erweitern Sie die `color_map` in `get_label_color()`
- **Output-Format**: √Ñndern Sie `quality` Parameter beim Speichern

### Fehlerbehebung

#### H√§ufige Probleme:

1. **"Frame-Bild nicht gefunden"**:
   - Frame-Pfade in JSON stimmen nicht mit tats√§chlichen Dateien √ºberein
   - Frame-Bilder wurden verschoben oder gel√∂scht

2. **"Fehler beim Parsen der JSON-Datei"**:
   - JSON-Datei ist besch√§digt oder unvollst√§ndig
   - Verwenden Sie einen JSON-Validator

3. **Schrift-Probleme**:
   - Script verwendet automatisch verf√ºgbare System-Schriften
   - Falls n√∂tig, installieren Sie zus√§tzliche Schriftarten

#### Debug-Modus:
F√ºgen Sie `print()` Statements hinzu, um den Verarbeitungsfortschritt zu verfolgen.

### Ergebnis-Beispiel

Nach der Ausf√ºhrung erhalten Sie:
```
annotated_Camera_Teich-20250907-155316-1757253196613-7_frames_detection_results/
‚îú‚îÄ‚îÄ annotated_frame_000390.jpg    # Hund mit gr√ºner Bounding-Box
‚îú‚îÄ‚îÄ annotated_frame_004710.jpg    # 3 Hunde-Detektionen
‚îú‚îÄ‚îÄ annotated_frame_004740.jpg    # 4 Katzen-Detektionen (orange)
‚îú‚îÄ‚îÄ ...
‚îî‚îÄ‚îÄ detection_summary.jpg         # √úbersicht der besten Detektionen
```

Jedes Bild zeigt die urspr√ºnglichen Frames mit:
- Farbige Bounding-Boxes um erkannte Objekte
- Label und Confidence-Score √ºber jeder Box
- Optimierte Lesbarkeit durch Hintergrund-Boxen
